{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate accelerate"
      ],
      "metadata": {
        "id": "fOyozOm9bBc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQFjxhfcAZGW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import numpy as np\n",
        "!pip install evaluate\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformers.__version__)\n",
        "print(transformers.__file__)"
      ],
      "metadata": {
        "id": "Ma18h3ZLVNQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"nkazi/SciEntsBank\")\n",
        "\n",
        "# use UA split (unseen answers)\n",
        "train_ds = dataset[\"train\"]\n",
        "test_ds = dataset[\"test_ua\"]"
      ],
      "metadata": {
        "id": "nuU1nKzoAvGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"distilbert-base-uncased\"\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) # tokenization 把句子切成单元词"
      ],
      "metadata": {
        "id": "7i0181SjA2mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess function: combine question + student_answer\n",
        "def preprocess(data):\n",
        "    # 把文本转换成了 input_ids, attention_mask, token_type_ids\n",
        "    return tokenizer(\n",
        "        data[\"question\"],\n",
        "        data[\"student_answer\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64\n",
        "    )\n",
        "\n",
        "encoded_train = train_ds.map(preprocess, batched=True) # 文字转成数字的token tokenization + encoding（把token词转成数字IDs）\n",
        "encoded_test = test_ds.map(preprocess, batched=True)"
      ],
      "metadata": {
        "id": "8bne7Cw3A_6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "num_labels = 5   # correct, wrong, partially_correct, irrelevant, contradictory\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ")"
      ],
      "metadata": {
        "id": "fPXmySTLBHhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy.compute(predictions=preds, references=labels),\n",
        "        \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\"),\n",
        "        \"f1_weighted\": f1.compute(predictions=preds, references=labels, average=\"weighted\"),\n",
        "    }"
      ],
      "metadata": {
        "id": "EkeF_GQ5BUao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "IQKldr3nCIck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/Misconception_Analysis/sci_bert_model\"\n",
        "\n"
      ],
      "metadata": {
        "id": "o9N_l87RCK2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=save_path,\n",
        "    overwrite_output_dir=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    save_steps=500,                  # 每500步保存一次\n",
        "    save_total_limit=2,              # 只保留最近2个checkpoint\n",
        "    eval_steps=500,                  # 每500步做一次eval\n",
        "    logging_steps=100,               # 每100步打印一次日志\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "jlSz3d6mar-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=encoded_train,\n",
        "    eval_dataset=encoded_test,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "k4d-rfKfBfVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train & Evaluate\n",
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "7oOdQfGrBoBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "BckLW-Wqk4Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation and Analysis\n",
        "\n",
        "The bert-base-uncased model was evaluated on the SciEntsBank dataset using accuracy and F1 scores. The results after three training epochs are summarized as follows:\n",
        "\n",
        "Evaluation Loss: 1.09\n",
        "\n",
        "Accuracy: 53.7%\n",
        "\n",
        "Macro F1: 0.375\n",
        "\n",
        "Weighted F1: 0.529\n",
        "\n",
        "These results suggest that the model performs substantially better than random guessing (baseline ≈ 20% with five classes), indicating that it has learned meaningful patterns in student responses. However, there are clear differences between the metrics:\n",
        "\n",
        "Accuracy vs. Macro F1: While overall accuracy is moderate (53.7%), the lower macro F1 score (0.375) reveals that the model struggles with minority classes, such as irrelevant or contradictory responses. This reflects the class imbalance in the dataset.\n",
        "\n",
        "Weighted F1: The weighted F1 score (0.529) is closer to accuracy, suggesting that performance is stronger on the more frequent classes (correct, wrong) but weaker on the less represented ones.\n",
        "\n",
        "From an educational perspective, this indicates that the model is relatively reliable at identifying clearly correct or incorrect responses but has more difficulty distinguishing categories such as partially correct and contradictory. Improving performance on these minority categories is critical for building systems that can effectively diagnose student misconceptions."
      ],
      "metadata": {
        "id": "qqq7s-tfGPaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
        "\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     \"/content/drive/MyDrive/Misconception_Analysis/sci_bert_model\"\n",
        "# )\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=args,\n",
        "#     train_dataset=encoded_train,\n",
        "#     eval_dataset=encoded_test,\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics,\n",
        "# )\n",
        "\n",
        "# trainer.train(resume_from_checkpoint=True)\n"
      ],
      "metadata": {
        "id": "nMzkhGKGYxuW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}